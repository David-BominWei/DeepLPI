{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "molembed_path = \"/home/wbm001/deeplpi/DeepLPI/data/mol_embed.csv\"\n",
    "seqembed_path = \"/home/wbm001/deeplpi/DeepLPI/data/seq_embed.csv\"\n",
    "train_path = \"/home/wbm001/deeplpi/DeepLPI/data/kd_train.csv\"\n",
    "test_path = \"/home/wbm001/deeplpi/DeepLPI/data/kd_test.csv\"\n",
    "tensorboard_path = \"/home/wbm001/deeplpi/DeepLPI/output/tensorboard/\"\n",
    "data_path = \"/home/wbm001/deeplpi/DeepLPI/output/\"\n",
    "\n",
    "RAMDOMSEED = 11\n",
    "CLASSIFYBOUND = -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "seqembed = pd.read_csv(seqembed_path,header=None)\n",
    "molembed = pd.read_csv(molembed_path,)\n",
    "train = pd.read_csv(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "molembed = molembed.set_index(\"0\")\n",
    "train[\"exist\"] = train[\"mol\"].map(lambda x : 1 if x in molembed.index.values else None)\n",
    "train = train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "from torch.utils.data import DataLoader,TensorDataset,SequentialSampler,RandomSampler\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, val = train_test_split(train, test_size=1000, random_state=RAMDOMSEED)\n",
    "\n",
    "# train\n",
    "train_seq = tensor(np.array(seqembed.loc[train[\"seq\"]])).to(torch.float32)\n",
    "train_mol = tensor(np.array(molembed.loc[train[\"mol\"]])).to(torch.float32)\n",
    "train_classify = tensor(np.array(train[\"pkd\"])).to(torch.float32)\n",
    "\n",
    "trainDataset = TensorDataset(train_mol,train_seq,train_classify)\n",
    "trainDataLoader = DataLoader(trainDataset, batch_size=256)\n",
    "\n",
    "#val\n",
    "val_seq = tensor(np.array(seqembed.loc[val[\"seq\"]])).to(torch.float32)\n",
    "val_mol = tensor(np.array(molembed.loc[val[\"mol\"]])).to(torch.float32)\n",
    "val_classify = tensor(np.array(val[\"pkd\"])).to(torch.float32)\n",
    "\n",
    "# valDataset = TensorDataset(val_mol,val_seq,val_classify)\n",
    "# valDataLoader = DataLoader(valDataset, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class resBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_conv1=False, strides=1, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.process = nn.Sequential (\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=strides, padding=1),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(out_channels)\n",
    "        )\n",
    "        \n",
    "        if use_conv1:\n",
    "            self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=strides)\n",
    "        else:\n",
    "            self.conv1 = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        left = self.process(x)\n",
    "        right = x if self.conv1 is None else self.conv1(x)\n",
    "        \n",
    "        return F.relu(left + right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnnModule(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, hidden_channel=32, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.head = nn.Sequential (\n",
    "            nn.Conv1d(in_channel, hidden_channel, 7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm1d(hidden_channel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        \n",
    "        self.cnn = nn.Sequential (\n",
    "            resBlock(hidden_channel, out_channel, use_conv1=True, strides=1),\n",
    "            resBlock(out_channel, out_channel, strides=1),\n",
    "            resBlock(out_channel, out_channel, strides=1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.head(x)\n",
    "        x = self.cnn(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLPI(nn.Module):\n",
    "    def __init__(self, molshape, seqshape, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.molshape = molshape\n",
    "        self.seqshape = seqshape\n",
    "\n",
    "        self.molcnn = cnnModule(1,16)\n",
    "        self.seqcnn = cnnModule(1,16)\n",
    "        \n",
    "        self.pool = nn.AvgPool1d(5, stride = 3)\n",
    "        self.lstm = nn.LSTM(16, 16, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.mlp = nn.Sequential (\n",
    "            nn.Linear(round(((300+6165)/4-2)*2/3) * 16, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            \n",
    "            nn.Linear(1024, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            \n",
    "            nn.Linear(128, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, mol, seq):\n",
    "        mol = self.molcnn(mol.reshape(-1,1,self.molshape))\n",
    "        seq = self.seqcnn(seq.reshape(-1,1,self.seqshape))\n",
    "        \n",
    "        # put data into lstm        \n",
    "        x = torch.cat((mol,seq),2)\n",
    "        x = self.pool(x)\n",
    "        # print(x.shape)\n",
    "        x = x.reshape(-1,round(((self.molshape+self.seqshape)/4-2)/3),16)\n",
    "\n",
    "        x,_ = self.lstm(x)\n",
    "        # fully connect layer\n",
    "        x = self.mlp(x.flatten(1))\n",
    "        \n",
    "        x = x.flatten()\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DeepLPI(300,6165)\n",
    "model(torch.randn(512,300),torch.randn(512,6165)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Conv1d):\n",
    "        nn.init.kaiming_uniform_(m.weight.data,nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "    elif isinstance(m, nn.BatchNorm1d):\n",
    "        nn.init.constant_(m.weight.data, 1)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight.data)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from matplotlib import pyplot as plt\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, train_dataloader, lossfunc, optimizer, scheduler):\n",
    "    model = model.to(\"cuda\")\n",
    "    model.train()\n",
    "    loop_loss = 0\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        step_mol, step_seq, step_label = batch\n",
    "        step_mol, step_seq, step_label = step_mol.to(\"cuda\"), step_seq.to(\"cuda\"), step_label.to(\"cuda\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(step_mol, step_seq)\n",
    "        loss = lossfunc(logits, step_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loop_loss += float(loss.to(\"cpu\"))\n",
    "\n",
    "        if step%20 == 0:\n",
    "            print(\"step \" + str(step) + \" loss: \" + str(float(loss.to(\"cpu\"))))\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        return loop_loss/len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "def test_loop(model, val_mol, val_seq, val_lab, writer, epoch):\n",
    "    model.eval()\n",
    "    model = model.to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        step_mol, step_seq = val_mol.to(\"cuda\"), val_seq.to(\"cuda\")\n",
    "        logits = model(step_mol,step_seq)\n",
    "    logits = logits.to(\"cpu\")\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    plt.xlabel(\"true value\")\n",
    "    plt.ylabel(\"predict value\")\n",
    "    plt.scatter(logits, val_lab, alpha = 0.2, color='Black')\n",
    "    plt.plot(range(-9,4), range(-9,4),color=\"r\",linewidth=2)\n",
    "    plt.xlim(-9,4)\n",
    "    plt.ylim(-9,4)\n",
    "    writer.add_figure(tag='test evaluate', figure=fig, global_step=epoch)\n",
    "\n",
    "    return mean_squared_error(val_lab,logits), r2_score(val_lab,logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = DeepLPI(300,6165)\n",
    "\n",
    "model.apply(initialize_weights)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.8, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "epoch: 0\n",
      "step 0 loss: 11.567291259765625\n",
      "step 20 loss: 4.658302307128906\n",
      "step 40 loss: 2.808175563812256\n",
      "step 60 loss: 2.871910572052002\n",
      "step 80 loss: 2.5707147121429443\n",
      "step 100 loss: 2.3985002040863037\n",
      "step 120 loss: 2.4781908988952637\n",
      "step 140 loss: 2.497664451599121\n",
      "step 160 loss: 2.2018117904663086\n",
      "step 180 loss: 1.9594147205352783\n",
      "\n",
      "R2: 0.035865593873574575\t MSE:2.5186098\n",
      "use time: 31.781075716018677\n",
      "----------------------------------------\n",
      "epoch: 1\n",
      "step 0 loss: 2.353210926055908\n",
      "step 20 loss: 2.3544228076934814\n",
      "step 40 loss: 2.0568456649780273\n",
      "step 60 loss: 1.7266076803207397\n",
      "step 80 loss: 1.833662509918213\n",
      "step 100 loss: 2.228581428527832\n",
      "step 120 loss: 2.047226905822754\n",
      "step 140 loss: 2.0754265785217285\n",
      "step 160 loss: 1.911547303199768\n",
      "step 180 loss: 2.013432025909424\n",
      "\n",
      "R2: 0.0750328781600984\t MSE:2.416293\n",
      "use time: 28.746172666549683\n",
      "----------------------------------------\n",
      "epoch: 2\n",
      "step 0 loss: 2.2740352153778076\n",
      "step 20 loss: 2.1114556789398193\n",
      "step 40 loss: 1.9837863445281982\n",
      "step 60 loss: 1.837424397468567\n",
      "step 80 loss: 1.906907081604004\n",
      "step 100 loss: 1.9037153720855713\n",
      "step 120 loss: 2.0492401123046875\n",
      "step 140 loss: 1.9343233108520508\n",
      "step 160 loss: 1.810765266418457\n",
      "step 180 loss: 1.657857060432434\n",
      "\n",
      "R2: 0.1038288593276726\t MSE:2.3410692\n",
      "use time: 28.872047662734985\n",
      "----------------------------------------\n",
      "epoch: 3\n",
      "step 0 loss: 2.081024169921875\n",
      "step 20 loss: 1.9481145143508911\n",
      "step 40 loss: 1.8962310552597046\n",
      "step 60 loss: 1.9170029163360596\n",
      "step 80 loss: 1.571371078491211\n",
      "step 100 loss: 1.8028504848480225\n",
      "step 120 loss: 1.8082726001739502\n",
      "step 140 loss: 1.8054518699645996\n",
      "step 160 loss: 1.8534352779388428\n",
      "step 180 loss: 1.4919899702072144\n",
      "\n",
      "R2: 0.03549127372732297\t MSE:2.5195875\n",
      "use time: 29.083234071731567\n",
      "----------------------------------------\n",
      "epoch: 4\n",
      "step 0 loss: 2.0681238174438477\n",
      "step 20 loss: 1.881347894668579\n",
      "step 40 loss: 1.7868202924728394\n",
      "step 60 loss: 1.5617992877960205\n",
      "step 80 loss: 1.573992133140564\n",
      "step 100 loss: 1.7908720970153809\n",
      "step 120 loss: 1.831120252609253\n",
      "step 140 loss: 1.6660737991333008\n",
      "step 160 loss: 1.8394386768341064\n",
      "step 180 loss: 1.4629318714141846\n",
      "\n",
      "R2: 0.27277550608805534\t MSE:1.8997296\n",
      "use time: 28.887401342391968\n",
      "----------------------------------------\n",
      "epoch: 5\n",
      "step 0 loss: 1.8463332653045654\n",
      "step 20 loss: 1.9161854982376099\n",
      "step 40 loss: 1.680812120437622\n",
      "step 60 loss: 1.7562694549560547\n",
      "step 80 loss: 1.337399959564209\n",
      "step 100 loss: 1.5956487655639648\n",
      "step 120 loss: 1.837884783744812\n",
      "step 140 loss: 1.6585015058517456\n",
      "step 160 loss: 1.7655518054962158\n",
      "step 180 loss: 1.4810972213745117\n",
      "\n",
      "R2: 0.2689302576557977\t MSE:1.9097747\n",
      "use time: 28.92373275756836\n",
      "----------------------------------------\n",
      "epoch: 6\n",
      "step 0 loss: 2.0064544677734375\n",
      "step 20 loss: 1.8270139694213867\n",
      "step 40 loss: 1.8123388290405273\n",
      "step 60 loss: 1.6169190406799316\n",
      "step 80 loss: 1.4623644351959229\n",
      "step 100 loss: 1.6005206108093262\n",
      "step 120 loss: 1.5531114339828491\n",
      "step 140 loss: 1.6354844570159912\n",
      "step 160 loss: 1.7187378406524658\n",
      "step 180 loss: 1.3309979438781738\n",
      "\n",
      "R2: 0.24813439450096375\t MSE:1.9640996\n",
      "use time: 29.006511926651\n",
      "----------------------------------------\n",
      "epoch: 7\n",
      "step 0 loss: 1.8938629627227783\n",
      "step 20 loss: 1.7926509380340576\n",
      "step 40 loss: 1.611490249633789\n",
      "step 60 loss: 1.5224764347076416\n",
      "step 80 loss: 1.5252599716186523\n",
      "step 100 loss: 1.5597648620605469\n",
      "step 120 loss: 1.5449464321136475\n",
      "step 140 loss: 1.6490669250488281\n",
      "step 160 loss: 1.7595274448394775\n",
      "step 180 loss: 1.2708324193954468\n",
      "\n",
      "R2: 0.3123738887582621\t MSE:1.7962866\n",
      "use time: 29.135474920272827\n",
      "----------------------------------------\n",
      "epoch: 8\n",
      "step 0 loss: 1.893481731414795\n",
      "step 20 loss: 1.6466069221496582\n",
      "step 40 loss: 1.722001314163208\n",
      "step 60 loss: 1.4515783786773682\n",
      "step 80 loss: 1.460259199142456\n",
      "step 100 loss: 1.4439544677734375\n",
      "step 120 loss: 1.5552825927734375\n",
      "step 140 loss: 1.6429939270019531\n",
      "step 160 loss: 1.6598199605941772\n",
      "step 180 loss: 1.222968578338623\n",
      "\n",
      "R2: 0.3366984513184288\t MSE:1.7327435\n",
      "use time: 29.08392858505249\n",
      "----------------------------------------\n",
      "epoch: 9\n",
      "step 0 loss: 1.780322790145874\n",
      "step 20 loss: 1.6783156394958496\n",
      "step 40 loss: 1.5269931554794312\n",
      "step 60 loss: 1.3269041776657104\n",
      "step 80 loss: 1.4030529260635376\n",
      "step 100 loss: 1.3252511024475098\n",
      "step 120 loss: 1.5810611248016357\n",
      "step 140 loss: 1.6860573291778564\n",
      "step 160 loss: 1.6372761726379395\n",
      "step 180 loss: 1.2787832021713257\n",
      "\n",
      "R2: 0.3097185652936961\t MSE:1.8032231\n",
      "use time: 28.88507080078125\n",
      "----------------------------------------\n",
      "epoch: 10\n",
      "step 0 loss: 1.760526180267334\n",
      "step 20 loss: 1.606139898300171\n",
      "step 40 loss: 1.4910531044006348\n",
      "step 60 loss: 1.422553300857544\n",
      "step 80 loss: 1.3130944967269897\n",
      "step 100 loss: 1.5083743333816528\n",
      "step 120 loss: 1.4125839471817017\n",
      "step 140 loss: 1.6254644393920898\n",
      "step 160 loss: 1.4474167823791504\n",
      "step 180 loss: 1.105074167251587\n",
      "\n",
      "R2: 0.3640192108671283\t MSE:1.6613735\n",
      "use time: 28.980803728103638\n",
      "----------------------------------------\n",
      "epoch: 11\n",
      "step 0 loss: 1.6072814464569092\n",
      "step 20 loss: 1.5009715557098389\n",
      "step 40 loss: 1.5951497554779053\n",
      "step 60 loss: 1.247492790222168\n",
      "step 80 loss: 1.3409746885299683\n",
      "step 100 loss: 1.3559718132019043\n",
      "step 120 loss: 1.4920083284378052\n",
      "step 140 loss: 1.5493136644363403\n",
      "step 160 loss: 1.5470757484436035\n",
      "step 180 loss: 1.1469041109085083\n",
      "\n",
      "R2: 0.37104585646813704\t MSE:1.6430178\n",
      "use time: 29.08274793624878\n",
      "----------------------------------------\n",
      "epoch: 12\n",
      "step 0 loss: 1.6012033224105835\n",
      "step 20 loss: 1.5515999794006348\n",
      "step 40 loss: 1.476130723953247\n",
      "step 60 loss: 1.3850122690200806\n",
      "step 80 loss: 1.2110956907272339\n",
      "step 100 loss: 1.3740812540054321\n",
      "step 120 loss: 1.5079545974731445\n",
      "step 140 loss: 1.528554916381836\n",
      "step 160 loss: 1.4402284622192383\n",
      "step 180 loss: 1.1133971214294434\n",
      "\n",
      "R2: 0.3896951499867479\t MSE:1.5943003\n",
      "use time: 28.914757013320923\n",
      "----------------------------------------\n",
      "epoch: 13\n",
      "step 0 loss: 1.6015625\n",
      "step 20 loss: 1.3024041652679443\n",
      "step 40 loss: 1.3509929180145264\n",
      "step 60 loss: 1.3750898838043213\n",
      "step 80 loss: 1.2694275379180908\n",
      "step 100 loss: 1.4767444133758545\n",
      "step 120 loss: 1.2932021617889404\n",
      "step 140 loss: 1.4004898071289062\n",
      "step 160 loss: 1.4676926136016846\n",
      "step 180 loss: 1.1206700801849365\n",
      "\n",
      "R2: 0.38025945725844534\t MSE:1.618949\n",
      "use time: 29.15818762779236\n",
      "----------------------------------------\n",
      "epoch: 14\n",
      "step 0 loss: 1.5252093076705933\n",
      "step 20 loss: 1.3519483804702759\n",
      "step 40 loss: 1.3466356992721558\n",
      "step 60 loss: 1.2449036836624146\n",
      "step 80 loss: 1.2144267559051514\n",
      "step 100 loss: 1.286717414855957\n",
      "step 120 loss: 1.3596277236938477\n",
      "step 140 loss: 1.527111291885376\n",
      "step 160 loss: 1.3689906597137451\n",
      "step 180 loss: 1.1335394382476807\n",
      "\n",
      "R2: 0.41955768377138913\t MSE:1.5162903\n",
      "use time: 29.01675581932068\n",
      "----------------------------------------\n",
      "epoch: 15\n",
      "step 0 loss: 1.505185842514038\n",
      "step 20 loss: 1.3426798582077026\n",
      "step 40 loss: 1.3433003425598145\n",
      "step 60 loss: 1.3062406778335571\n",
      "step 80 loss: 1.241513729095459\n",
      "step 100 loss: 1.2707674503326416\n",
      "step 120 loss: 1.2624703645706177\n",
      "step 140 loss: 1.6418646574020386\n",
      "step 160 loss: 1.383556842803955\n",
      "step 180 loss: 1.134777545928955\n",
      "\n",
      "R2: 0.39065240653568933\t MSE:1.5917996\n",
      "use time: 29.09904670715332\n",
      "----------------------------------------\n",
      "epoch: 16\n",
      "step 0 loss: 1.5432831048965454\n",
      "step 20 loss: 1.3182573318481445\n",
      "step 40 loss: 1.4049944877624512\n",
      "step 60 loss: 1.3379005193710327\n",
      "step 80 loss: 1.2442070245742798\n",
      "step 100 loss: 1.1984165906906128\n",
      "step 120 loss: 1.2331674098968506\n",
      "step 140 loss: 1.528368592262268\n",
      "step 160 loss: 1.389415979385376\n",
      "step 180 loss: 1.0751819610595703\n",
      "\n",
      "R2: 0.4298661538753076\t MSE:1.4893613\n",
      "use time: 28.92274761199951\n",
      "----------------------------------------\n",
      "epoch: 17\n",
      "step 0 loss: 1.484961986541748\n",
      "step 20 loss: 1.451413631439209\n",
      "step 40 loss: 1.5358014106750488\n",
      "step 60 loss: 1.3032708168029785\n",
      "step 80 loss: 1.1326806545257568\n",
      "step 100 loss: 1.200893521308899\n",
      "step 120 loss: 1.2683383226394653\n",
      "step 140 loss: 1.4169071912765503\n",
      "step 160 loss: 1.3063620328903198\n",
      "step 180 loss: 0.9770528674125671\n",
      "\n",
      "R2: 0.4614752296630589\t MSE:1.4067891\n",
      "use time: 28.900445222854614\n",
      "----------------------------------------\n",
      "epoch: 18\n",
      "step 0 loss: 1.5152246952056885\n",
      "step 20 loss: 1.5236899852752686\n",
      "step 40 loss: 1.2991729974746704\n",
      "step 60 loss: 1.3057786226272583\n",
      "step 80 loss: 1.120732307434082\n",
      "step 100 loss: 1.2417906522750854\n",
      "step 120 loss: 1.2853825092315674\n",
      "step 140 loss: 1.3056082725524902\n",
      "step 160 loss: 1.4038596153259277\n",
      "step 180 loss: 1.0597516298294067\n",
      "\n",
      "R2: 0.46007562507377076\t MSE:1.4104453\n",
      "use time: 29.03812265396118\n",
      "----------------------------------------\n",
      "epoch: 19\n",
      "step 0 loss: 1.4473402500152588\n",
      "step 20 loss: 1.3340907096862793\n",
      "step 40 loss: 1.2696210145950317\n",
      "step 60 loss: 1.3646113872528076\n",
      "step 80 loss: 1.083034873008728\n",
      "step 100 loss: 1.224263072013855\n",
      "step 120 loss: 1.2783191204071045\n",
      "step 140 loss: 1.3981205224990845\n",
      "step 160 loss: 1.2797598838806152\n",
      "step 180 loss: 0.9302337169647217\n",
      "\n",
      "R2: 0.46941085895129786\t MSE:1.3860587\n",
      "use time: 28.9675076007843\n",
      "----------------------------------------\n",
      "epoch: 20\n",
      "step 0 loss: 1.348738193511963\n",
      "step 20 loss: 1.2008204460144043\n",
      "step 40 loss: 1.1800038814544678\n",
      "step 60 loss: 1.3491493463516235\n",
      "step 80 loss: 1.0618057250976562\n",
      "step 100 loss: 1.1194497346878052\n",
      "step 120 loss: 1.1898962259292603\n",
      "step 140 loss: 1.392728567123413\n",
      "step 160 loss: 1.2337076663970947\n",
      "step 180 loss: 0.9057263135910034\n",
      "\n",
      "R2: 0.48585886080137064\t MSE:1.3430917\n",
      "use time: 29.041342973709106\n",
      "----------------------------------------\n",
      "epoch: 21\n",
      "step 0 loss: 1.4129951000213623\n",
      "step 20 loss: 1.2997946739196777\n",
      "step 40 loss: 1.291873574256897\n",
      "step 60 loss: 1.212964415550232\n",
      "step 80 loss: 1.047745943069458\n",
      "step 100 loss: 1.17325758934021\n",
      "step 120 loss: 1.2412610054016113\n",
      "step 140 loss: 1.4423015117645264\n",
      "step 160 loss: 1.2754801511764526\n",
      "step 180 loss: 1.0536495447158813\n",
      "\n",
      "R2: 0.48091745826997345\t MSE:1.356\n",
      "use time: 28.93540596961975\n",
      "----------------------------------------\n",
      "epoch: 22\n",
      "step 0 loss: 1.3245199918746948\n",
      "step 20 loss: 1.186953067779541\n",
      "step 40 loss: 1.2280240058898926\n",
      "step 60 loss: 1.2068533897399902\n",
      "step 80 loss: 1.0412465333938599\n",
      "step 100 loss: 1.1211812496185303\n",
      "step 120 loss: 1.2801294326782227\n",
      "step 140 loss: 1.1851954460144043\n",
      "step 160 loss: 1.232896089553833\n",
      "step 180 loss: 0.9455227255821228\n",
      "\n",
      "R2: 0.46812447178384486\t MSE:1.3894192\n",
      "use time: 28.932173490524292\n",
      "----------------------------------------\n",
      "epoch: 23\n",
      "step 0 loss: 1.3863537311553955\n",
      "step 20 loss: 1.2760603427886963\n",
      "step 40 loss: 1.2372310161590576\n",
      "step 60 loss: 1.1822301149368286\n",
      "step 80 loss: 1.0937974452972412\n",
      "step 100 loss: 1.0662522315979004\n",
      "step 120 loss: 1.2723965644836426\n",
      "step 140 loss: 1.3815900087356567\n",
      "step 160 loss: 1.2005743980407715\n",
      "step 180 loss: 0.9544199109077454\n",
      "\n",
      "R2: 0.4583525562430574\t MSE:1.4149463\n",
      "use time: 28.970343351364136\n",
      "----------------------------------------\n",
      "epoch: 24\n",
      "step 0 loss: 1.3717923164367676\n",
      "step 20 loss: 1.3606150150299072\n",
      "step 40 loss: 1.2762432098388672\n",
      "step 60 loss: 1.2303680181503296\n",
      "step 80 loss: 1.084321141242981\n",
      "step 100 loss: 1.161025047302246\n",
      "step 120 loss: 1.1853787899017334\n",
      "step 140 loss: 1.2677104473114014\n",
      "step 160 loss: 1.093614935874939\n",
      "step 180 loss: 0.9223382472991943\n",
      "\n",
      "R2: 0.49519830516972796\t MSE:1.3186941\n",
      "use time: 29.107474088668823\n",
      "----------------------------------------\n",
      "epoch: 25\n",
      "step 0 loss: 1.3778789043426514\n",
      "step 20 loss: 1.2439067363739014\n",
      "step 40 loss: 1.2323976755142212\n",
      "step 60 loss: 1.1270699501037598\n",
      "step 80 loss: 1.0134073495864868\n",
      "step 100 loss: 1.0973246097564697\n",
      "step 120 loss: 1.1789162158966064\n",
      "step 140 loss: 1.2765347957611084\n",
      "step 160 loss: 1.1419572830200195\n",
      "step 180 loss: 0.9293466806411743\n",
      "\n",
      "R2: 0.5106622303248429\t MSE:1.2782979\n",
      "use time: 29.09548830986023\n",
      "----------------------------------------\n",
      "epoch: 26\n",
      "step 0 loss: 1.349733591079712\n",
      "step 20 loss: 1.1710447072982788\n",
      "step 40 loss: 1.1654231548309326\n",
      "step 60 loss: 1.149200439453125\n",
      "step 80 loss: 1.1137373447418213\n",
      "step 100 loss: 1.112457275390625\n",
      "step 120 loss: 1.1551344394683838\n",
      "step 140 loss: 1.2038875818252563\n",
      "step 160 loss: 1.064198613166809\n",
      "step 180 loss: 0.9698251485824585\n",
      "\n",
      "R2: 0.5364683649105353\t MSE:1.2108843\n",
      "use time: 28.91144299507141\n",
      "----------------------------------------\n",
      "epoch: 27\n",
      "step 0 loss: 1.236049771308899\n",
      "step 20 loss: 1.1606066226959229\n",
      "step 40 loss: 1.1469287872314453\n",
      "step 60 loss: 1.1866313219070435\n",
      "step 80 loss: 1.0135592222213745\n",
      "step 100 loss: 1.013737440109253\n",
      "step 120 loss: 1.1022014617919922\n",
      "step 140 loss: 1.2333500385284424\n",
      "step 160 loss: 1.1649543046951294\n",
      "step 180 loss: 0.9576653242111206\n",
      "\n",
      "R2: 0.48429639904197985\t MSE:1.3471732\n",
      "use time: 28.873393297195435\n",
      "----------------------------------------\n",
      "epoch: 28\n",
      "step 0 loss: 1.3113678693771362\n",
      "step 20 loss: 1.1581299304962158\n",
      "step 40 loss: 1.1920487880706787\n",
      "step 60 loss: 1.205411672592163\n",
      "step 80 loss: 1.0243548154830933\n",
      "step 100 loss: 1.0646207332611084\n",
      "step 120 loss: 1.1646051406860352\n",
      "step 140 loss: 1.1918106079101562\n",
      "step 160 loss: 1.1118168830871582\n",
      "step 180 loss: 0.8770278096199036\n",
      "\n",
      "R2: 0.5390975555875952\t MSE:1.2040161\n",
      "use time: 28.986748456954956\n",
      "----------------------------------------\n",
      "epoch: 29\n",
      "step 0 loss: 1.2349233627319336\n",
      "step 20 loss: 1.1966215372085571\n",
      "step 40 loss: 1.177993893623352\n",
      "step 60 loss: 1.1539337635040283\n",
      "step 80 loss: 0.9654663801193237\n",
      "step 100 loss: 0.9823460578918457\n",
      "step 120 loss: 1.0610696077346802\n",
      "step 140 loss: 1.1425426006317139\n",
      "step 160 loss: 1.0963866710662842\n",
      "step 180 loss: 0.8708810210227966\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-9caa521cc7a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtime0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mavgloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mmsescore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr2score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_mol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_classify\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-bfc11eee6127>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(model, train_dataloader, lossfunc, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mloop_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wbm001/miniconda3/envs/DeepPurpose/lib/python3.6/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wbm001/miniconda3/envs/DeepPurpose/lib/python3.6/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "\n",
    "version = \"v9b1reg\"\n",
    "\n",
    "writer = SummaryWriter(tensorboard_path)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    print(\"--\"*20)\n",
    "    print(\"epoch: \" + str(epoch))\n",
    "    time0 = time.time()\n",
    "\n",
    "    avgloss = train_loop(model, trainDataLoader, loss_fn, optimizer, scheduler)\n",
    "    msescore, r2score = test_loop(model, val_mol, val_seq, val_classify, writer, epoch)\n",
    "\n",
    "    writer.add_scalar(\"test time\", time.time()-time0, epoch)\n",
    "    writer.add_scalar('avgloss', avgloss , epoch)\n",
    "    writer.add_scalar('mse', msescore , epoch)\n",
    "    writer.add_scalar('r2', r2score , epoch)\n",
    "    writer.add_scalar('current lr', optimizer.param_groups[0]['lr'], epoch)\n",
    "\n",
    "    print()\n",
    "    print(\"R2: \" + str(r2score) + \"\\t MSE: \" + str(msescore))\n",
    "    print(\"use time: \" + str(time.time() - time0))\n",
    "    \n",
    "    model.eval()\n",
    "    if epoch % 50 == 0:\n",
    "        torch.save({'state_dict': model.state_dict()}, data_path + 'model/' + str(version) + \"e\" + str(epoch) + '.pth.tar')\n",
    "    else:\n",
    "        torch.save({'state_dict': model.state_dict()}, data_path + \"model/quicksave.pth.tar\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('DeepPurpose')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f923ce3d9a6c852fa4277c7633c15be33e3c2fd747753029b4f9b6323264f49d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
