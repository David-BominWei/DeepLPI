{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "molembed_path = \"/home/wbm001/deeplpi/DeepLPI/data/mol_embed.csv\"\n",
    "seqembed_path = \"/home/wbm001/deeplpi/DeepLPI/data/seq_embed.csv\"\n",
    "train_path = \"/home/wbm001/deeplpi/DeepLPI/data/kd_train.csv\"\n",
    "test_path = \"/home/wbm001/deeplpi/DeepLPI/data/kd_test.csv\"\n",
    "tensorboard_path = \"/home/wbm001/deeplpi/DeepLPI/output/tensorboard/\"\n",
    "data_path = \"/home/wbm001/deeplpi/DeepLPI/output/\"\n",
    "\n",
    "RAMDOMSEED = 11\n",
    "CLASSIFYBOUND = -2\n",
    "version = \"v9b3reg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "seqembed = pd.read_csv(seqembed_path,header=None)\n",
    "molembed = pd.read_csv(molembed_path,)\n",
    "train = pd.read_csv(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "molembed = molembed.set_index(\"0\")\n",
    "train[\"exist\"] = train[\"mol\"].map(lambda x : 1 if x in molembed.index.values else None)\n",
    "train = train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "from torch.utils.data import DataLoader,TensorDataset,SequentialSampler,RandomSampler\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, val = train_test_split(train, test_size=1000, random_state=RAMDOMSEED)\n",
    "\n",
    "# train\n",
    "train_seq = tensor(np.array(seqembed.loc[train[\"seq\"]])).to(torch.float32)\n",
    "train_mol = tensor(np.array(molembed.loc[train[\"mol\"]])).to(torch.float32)\n",
    "train_classify = tensor(np.array(train[\"pkd\"])).to(torch.float32)\n",
    "\n",
    "trainDataset = TensorDataset(train_mol,train_seq,train_classify)\n",
    "trainDataLoader = DataLoader(trainDataset, batch_size=256)\n",
    "\n",
    "#val\n",
    "val_seq = tensor(np.array(seqembed.loc[val[\"seq\"]])).to(torch.float32)\n",
    "val_mol = tensor(np.array(molembed.loc[val[\"mol\"]])).to(torch.float32)\n",
    "val_classify = tensor(np.array(val[\"pkd\"])).to(torch.float32)\n",
    "\n",
    "# valDataset = TensorDataset(val_mol,val_seq,val_classify)\n",
    "# valDataLoader = DataLoader(valDataset, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class resBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_conv1=False, strides=1, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.process = nn.Sequential (\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=strides, padding=1),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(out_channels)\n",
    "        )\n",
    "        \n",
    "        if use_conv1:\n",
    "            self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=strides)\n",
    "        else:\n",
    "            self.conv1 = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        left = self.process(x)\n",
    "        right = x if self.conv1 is None else self.conv1(x)\n",
    "        \n",
    "        return F.relu(left + right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnnModule(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, hidden_channel=32, dropout=0.3, headpooling=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head = nn.Sequential (\n",
    "            nn.Conv1d(in_channel, hidden_channel, 7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm1d(hidden_channel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.MaxPool1d(5, stride=3) if headpooling == True else nn.MaxPool1d(2)\n",
    "        )\n",
    "        \n",
    "        self.cnn = nn.Sequential (\n",
    "            resBlock(hidden_channel, out_channel, use_conv1=True, strides=1),\n",
    "            resBlock(out_channel, out_channel, strides=1),\n",
    "            resBlock(out_channel, out_channel, strides=1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.head(x)\n",
    "        x = self.cnn(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLPI(nn.Module):\n",
    "    def __init__(self, molshape, seqshape, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.molshape = molshape\n",
    "        self.seqshape = seqshape\n",
    "\n",
    "        self.molcnn = cnnModule(1, 16, dropout=dropout)\n",
    "        self.seqcnn = cnnModule(1, 16, dropout=dropout, headpooling=True)\n",
    "        \n",
    "        self.pool = nn.AvgPool1d(7, stride = 5, padding=2)\n",
    "        self.lstm = nn.LSTM(16, 16, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.mlp = nn.Sequential (\n",
    "            nn.Linear(round(self.molshape/4+self.seqshape/30) * 16 * 2, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            \n",
    "            nn.Linear(1024, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            \n",
    "            nn.Linear(128, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, mol, seq):\n",
    "        mol = self.molcnn(mol.reshape(-1,1,self.molshape))\n",
    "        seq = self.seqcnn(seq.reshape(-1,1,self.seqshape))\n",
    "        \n",
    "        # put data into lstm\n",
    "        seq = self.pool(seq)\n",
    "        # print(mol.shape,seq.shape)\n",
    "        x = torch.cat((mol,seq),2)\n",
    "        # print(mol.shape)\n",
    "        x = x.reshape(-1,round(self.molshape/4+self.seqshape/30),16)\n",
    "\n",
    "        x,_ = self.lstm(x)\n",
    "        # fully connect layer\n",
    "        # print(x.shape)\n",
    "        x = self.mlp(x.flatten(1))\n",
    "        \n",
    "        x = x.flatten()\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DeepLPI(300,6165,dropout=0.2)\n",
    "model(torch.randn(512,300),torch.randn(512,6165)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Conv1d):\n",
    "        nn.init.kaiming_uniform_(m.weight.data,nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "    elif isinstance(m, nn.BatchNorm1d):\n",
    "        nn.init.constant_(m.weight.data, 1)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight.data)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from matplotlib import pyplot as plt\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, train_dataloader, lossfunc, optimizer, scheduler):\n",
    "    model = model.to(\"cuda\")\n",
    "    model.train()\n",
    "    loop_loss = 0\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        step_mol, step_seq, step_label = batch\n",
    "        step_mol, step_seq, step_label = step_mol.to(\"cuda\"), step_seq.to(\"cuda\"), step_label.to(\"cuda\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(step_mol, step_seq)\n",
    "        loss = lossfunc(logits, step_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loop_loss += float(loss.to(\"cpu\"))\n",
    "\n",
    "        if step%20 == 0:\n",
    "            print(\"step \" + str(step) + \" loss: \" + str(float(loss.to(\"cpu\"))))\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        return loop_loss/len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "def test_loop(model, val_mol, val_seq, val_lab, writer, epoch):\n",
    "    model.eval()\n",
    "    model = model.to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        step_mol, step_seq = val_mol.to(\"cuda\"), val_seq.to(\"cuda\")\n",
    "        logits = model(step_mol,step_seq)\n",
    "    logits = logits.to(\"cpu\")\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    plt.xlabel(\"true value\")\n",
    "    plt.ylabel(\"predict value\")\n",
    "    plt.scatter(logits, val_lab, alpha = 0.2, color='Black')\n",
    "    plt.plot(range(-9,4), range(-9,4),color=\"r\",linewidth=2)\n",
    "    plt.xlim(-9,4)\n",
    "    plt.ylim(-9,4)\n",
    "    writer.add_figure(tag='test evaluate', figure=fig, global_step=epoch)\n",
    "\n",
    "    return mean_squared_error(val_lab,logits), r2_score(val_lab,logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = DeepLPI(300,6165,dropout=0.2)\n",
    "\n",
    "model.apply(initialize_weights)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.8, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "epoch: 0\n",
      "step 0 loss: 11.724775314331055\n",
      "step 20 loss: 3.135772466659546\n",
      "step 40 loss: 2.1065382957458496\n",
      "step 60 loss: 2.0474023818969727\n",
      "step 80 loss: 2.2985334396362305\n",
      "step 100 loss: 2.507981300354004\n",
      "step 120 loss: 2.160972833633423\n",
      "step 140 loss: 1.8863177299499512\n",
      "step 160 loss: 2.0853381156921387\n",
      "step 180 loss: 1.8685121536254883\n",
      "\n",
      "R2: -0.28056095372419176\t MSE: 3.3452113\n",
      "use time: 21.705047845840454\n",
      "----------------------------------------\n",
      "epoch: 1\n",
      "step 0 loss: 2.1609222888946533\n",
      "step 20 loss: 1.9458903074264526\n",
      "step 40 loss: 1.8047432899475098\n",
      "step 60 loss: 1.747086524963379\n",
      "step 80 loss: 1.5375871658325195\n",
      "step 100 loss: 1.9318115711212158\n",
      "step 120 loss: 1.8560770750045776\n",
      "step 140 loss: 1.683241367340088\n",
      "step 160 loss: 1.7595782279968262\n",
      "step 180 loss: 1.5550227165222168\n",
      "\n",
      "R2: 0.2999822788040841\t MSE: 1.8286574\n",
      "use time: 21.55266284942627\n",
      "----------------------------------------\n",
      "epoch: 2\n",
      "step 0 loss: 1.8136930465698242\n",
      "step 20 loss: 1.8802673816680908\n",
      "step 40 loss: 1.6038795709609985\n",
      "step 60 loss: 1.5419185161590576\n",
      "step 80 loss: 1.3364185094833374\n",
      "step 100 loss: 1.567756175994873\n",
      "step 120 loss: 1.757959246635437\n",
      "step 140 loss: 1.6569852828979492\n",
      "step 160 loss: 1.6064603328704834\n",
      "step 180 loss: 1.4020808935165405\n",
      "\n",
      "R2: 0.38501682852252295\t MSE: 1.6065215\n",
      "use time: 21.722604274749756\n",
      "----------------------------------------\n",
      "epoch: 3\n",
      "step 0 loss: 1.7566810846328735\n",
      "step 20 loss: 1.5102834701538086\n",
      "step 40 loss: 1.4876680374145508\n",
      "step 60 loss: 1.504186749458313\n",
      "step 80 loss: 1.3846909999847412\n",
      "step 100 loss: 1.4764684438705444\n",
      "step 120 loss: 1.4387794733047485\n",
      "step 140 loss: 1.4998254776000977\n",
      "step 160 loss: 1.535973072052002\n",
      "step 180 loss: 1.2246968746185303\n",
      "\n",
      "R2: 0.4223065633540223\t MSE: 1.5091094\n",
      "use time: 21.675747632980347\n",
      "----------------------------------------\n",
      "epoch: 4\n",
      "step 0 loss: 1.7086824178695679\n",
      "step 20 loss: 1.3560084104537964\n",
      "step 40 loss: 1.5681382417678833\n",
      "step 60 loss: 1.3719089031219482\n",
      "step 80 loss: 1.0874069929122925\n",
      "step 100 loss: 1.313227653503418\n",
      "step 120 loss: 1.3286420106887817\n",
      "step 140 loss: 1.5659561157226562\n",
      "step 160 loss: 1.2863726615905762\n",
      "step 180 loss: 1.1459276676177979\n",
      "\n",
      "R2: 0.45588717469124496\t MSE: 1.4213867\n",
      "use time: 21.891916036605835\n",
      "----------------------------------------\n",
      "epoch: 5\n",
      "step 0 loss: 1.5968999862670898\n",
      "step 20 loss: 1.5352652072906494\n",
      "step 40 loss: 1.577958583831787\n",
      "step 60 loss: 1.3034406900405884\n",
      "step 80 loss: 1.1328654289245605\n",
      "step 100 loss: 1.226456880569458\n",
      "step 120 loss: 1.1561540365219116\n",
      "step 140 loss: 1.347544550895691\n",
      "step 160 loss: 1.2949312925338745\n",
      "step 180 loss: 0.9975131750106812\n",
      "\n",
      "R2: 0.5053524528873188\t MSE: 1.2921685\n",
      "use time: 21.694650888442993\n",
      "----------------------------------------\n",
      "epoch: 6\n",
      "step 0 loss: 1.5533679723739624\n",
      "step 20 loss: 1.2600340843200684\n",
      "step 40 loss: 1.287623643875122\n",
      "step 60 loss: 1.2514171600341797\n",
      "step 80 loss: 1.167128086090088\n",
      "step 100 loss: 1.117478609085083\n",
      "step 120 loss: 1.2669262886047363\n",
      "step 140 loss: 1.1986665725708008\n",
      "step 160 loss: 1.1749569177627563\n",
      "step 180 loss: 1.0004299879074097\n",
      "\n",
      "R2: 0.4439801038910518\t MSE: 1.4524915\n",
      "use time: 21.71839427947998\n",
      "----------------------------------------\n",
      "epoch: 7\n",
      "step 0 loss: 1.4310886859893799\n",
      "step 20 loss: 1.2164397239685059\n",
      "step 40 loss: 1.254664421081543\n",
      "step 60 loss: 1.2433648109436035\n",
      "step 80 loss: 1.0179526805877686\n",
      "step 100 loss: 1.1032159328460693\n",
      "step 120 loss: 1.2831623554229736\n",
      "step 140 loss: 1.169590950012207\n",
      "step 160 loss: 1.0467479228973389\n",
      "step 180 loss: 0.8852905035018921\n",
      "\n",
      "R2: 0.44994086295696945\t MSE: 1.4369203\n",
      "use time: 21.651114225387573\n",
      "----------------------------------------\n",
      "epoch: 8\n",
      "step 0 loss: 1.3976762294769287\n",
      "step 20 loss: 1.1804472208023071\n",
      "step 40 loss: 1.2137160301208496\n",
      "step 60 loss: 1.0851879119873047\n",
      "step 80 loss: 0.9597181081771851\n",
      "step 100 loss: 1.1334069967269897\n",
      "step 120 loss: 1.1548473834991455\n",
      "step 140 loss: 1.1971455812454224\n",
      "step 160 loss: 1.1192659139633179\n",
      "step 180 loss: 0.9495470523834229\n",
      "\n",
      "R2: 0.4988727779872627\t MSE: 1.3090954\n",
      "use time: 21.610103845596313\n",
      "----------------------------------------\n",
      "epoch: 9\n",
      "step 0 loss: 1.2629244327545166\n",
      "step 20 loss: 1.1684973239898682\n",
      "step 40 loss: 1.2260340452194214\n",
      "step 60 loss: 1.2055878639221191\n",
      "step 80 loss: 0.921678364276886\n",
      "step 100 loss: 1.0206031799316406\n",
      "step 120 loss: 1.2340258359909058\n",
      "step 140 loss: 1.1452994346618652\n",
      "step 160 loss: 1.0302581787109375\n",
      "step 180 loss: 0.9445156455039978\n",
      "\n",
      "R2: 0.5464949290832972\t MSE: 1.1846919\n",
      "use time: 21.606093883514404\n",
      "----------------------------------------\n",
      "epoch: 10\n",
      "step 0 loss: 1.2915828227996826\n",
      "step 20 loss: 1.1948738098144531\n",
      "step 40 loss: 1.119821310043335\n",
      "step 60 loss: 1.1363084316253662\n",
      "step 80 loss: 0.8868712782859802\n",
      "step 100 loss: 0.9455858469009399\n",
      "step 120 loss: 1.072823405265808\n",
      "step 140 loss: 1.0449031591415405\n",
      "step 160 loss: 1.0330111980438232\n",
      "step 180 loss: 0.9193294644355774\n",
      "\n",
      "R2: 0.5742232970217878\t MSE: 1.1122571\n",
      "use time: 21.75486445426941\n",
      "----------------------------------------\n",
      "epoch: 11\n",
      "step 0 loss: 1.2087047100067139\n",
      "step 20 loss: 1.055009126663208\n",
      "step 40 loss: 1.059006929397583\n",
      "step 60 loss: 1.031328797340393\n",
      "step 80 loss: 0.9483055472373962\n",
      "step 100 loss: 0.8175294995307922\n",
      "step 120 loss: 1.1353511810302734\n",
      "step 140 loss: 1.0349130630493164\n",
      "step 160 loss: 0.9592607021331787\n",
      "step 180 loss: 0.7722316980361938\n",
      "\n",
      "R2: 0.5692889685406299\t MSE: 1.125147\n",
      "use time: 21.606554985046387\n",
      "----------------------------------------\n",
      "epoch: 12\n",
      "step 0 loss: 1.0226795673370361\n",
      "step 20 loss: 1.1173222064971924\n",
      "step 40 loss: 1.0326755046844482\n",
      "step 60 loss: 1.0956895351409912\n",
      "step 80 loss: 0.8218289017677307\n",
      "step 100 loss: 0.9456695318222046\n",
      "step 120 loss: 1.1238614320755005\n",
      "step 140 loss: 1.0562293529510498\n",
      "step 160 loss: 0.8267157077789307\n",
      "step 180 loss: 0.7798548340797424\n",
      "\n",
      "R2: 0.5484539050599031\t MSE: 1.1795745\n",
      "use time: 21.60164499282837\n",
      "----------------------------------------\n",
      "epoch: 13\n",
      "step 0 loss: 1.1503185033798218\n",
      "step 20 loss: 1.0336754322052002\n",
      "step 40 loss: 1.1081173419952393\n",
      "step 60 loss: 0.9892818927764893\n",
      "step 80 loss: 0.8900226950645447\n",
      "step 100 loss: 0.8927686810493469\n",
      "step 120 loss: 1.0851800441741943\n",
      "step 140 loss: 0.9314639568328857\n",
      "step 160 loss: 0.8561612367630005\n",
      "step 180 loss: 0.8116595149040222\n",
      "\n",
      "R2: 0.5465222599131119\t MSE: 1.1846206\n",
      "use time: 21.657613515853882\n",
      "----------------------------------------\n",
      "epoch: 14\n",
      "step 0 loss: 1.0359691381454468\n",
      "step 20 loss: 1.141945719718933\n",
      "step 40 loss: 1.0548818111419678\n",
      "step 60 loss: 1.0285358428955078\n",
      "step 80 loss: 0.8813801407814026\n",
      "step 100 loss: 0.8354971408843994\n",
      "step 120 loss: 0.990786075592041\n",
      "step 140 loss: 0.9501428604125977\n",
      "step 160 loss: 0.8305085897445679\n",
      "step 180 loss: 0.787225604057312\n",
      "\n",
      "R2: 0.526737864582097\t MSE: 1.2363033\n",
      "use time: 21.60995841026306\n",
      "----------------------------------------\n",
      "epoch: 15\n",
      "step 0 loss: 1.0180474519729614\n",
      "step 20 loss: 1.0013108253479004\n",
      "step 40 loss: 0.9279506802558899\n",
      "step 60 loss: 0.9956145882606506\n",
      "step 80 loss: 0.8649219274520874\n",
      "step 100 loss: 0.8291374444961548\n",
      "step 120 loss: 1.0454261302947998\n",
      "step 140 loss: 1.0292835235595703\n",
      "step 160 loss: 0.8058951497077942\n",
      "step 180 loss: 0.8117607235908508\n",
      "\n",
      "R2: 0.61639839656931\t MSE: 1.0020831\n",
      "use time: 21.778812408447266\n",
      "----------------------------------------\n",
      "epoch: 16\n",
      "step 0 loss: 1.0305086374282837\n",
      "step 20 loss: 0.934051513671875\n",
      "step 40 loss: 0.9722607135772705\n",
      "step 60 loss: 0.9366390705108643\n",
      "step 80 loss: 0.6777974367141724\n",
      "step 100 loss: 0.8984056711196899\n",
      "step 120 loss: 1.0572452545166016\n",
      "step 140 loss: 0.9382930994033813\n",
      "step 160 loss: 0.7245984077453613\n",
      "step 180 loss: 0.6820136308670044\n",
      "\n",
      "R2: 0.6057817384586579\t MSE: 1.0298169\n",
      "use time: 21.66408133506775\n",
      "----------------------------------------\n",
      "epoch: 17\n",
      "step 0 loss: 0.9862815141677856\n",
      "step 20 loss: 0.938538670539856\n",
      "step 40 loss: 1.008538842201233\n",
      "step 60 loss: 0.901701807975769\n",
      "step 80 loss: 0.785599946975708\n",
      "step 100 loss: 0.9078576564788818\n",
      "step 120 loss: 0.9300397634506226\n",
      "step 140 loss: 0.9150098562240601\n",
      "step 160 loss: 0.7545322179794312\n",
      "step 180 loss: 0.6624618172645569\n",
      "\n",
      "R2: 0.5994450982627315\t MSE: 1.0463701\n",
      "use time: 21.619674682617188\n",
      "----------------------------------------\n",
      "epoch: 18\n",
      "step 0 loss: 1.0530297756195068\n",
      "step 20 loss: 0.8339959383010864\n",
      "step 40 loss: 1.0726503133773804\n",
      "step 60 loss: 0.9975159168243408\n",
      "step 80 loss: 0.7646715044975281\n",
      "step 100 loss: 0.8594586849212646\n",
      "step 120 loss: 0.9866131544113159\n",
      "step 140 loss: 0.907244086265564\n",
      "step 160 loss: 0.6857734322547913\n",
      "step 180 loss: 0.6587303876876831\n",
      "\n",
      "R2: 0.6138039206084601\t MSE: 1.0088606\n",
      "use time: 21.62591314315796\n",
      "----------------------------------------\n",
      "epoch: 19\n",
      "step 0 loss: 0.946141242980957\n",
      "step 20 loss: 0.9412121772766113\n",
      "step 40 loss: 0.8936821222305298\n",
      "step 60 loss: 0.8930537700653076\n",
      "step 80 loss: 0.708595871925354\n",
      "step 100 loss: 0.804111123085022\n",
      "step 120 loss: 0.930888831615448\n",
      "step 140 loss: 0.8498964309692383\n",
      "step 160 loss: 0.7488716840744019\n",
      "step 180 loss: 0.6867448091506958\n",
      "\n",
      "R2: 0.5970780277509901\t MSE: 1.0525537\n",
      "use time: 21.771594762802124\n",
      "----------------------------------------\n",
      "epoch: 20\n",
      "step 0 loss: 0.9291431307792664\n",
      "step 20 loss: 0.8109598159790039\n",
      "step 40 loss: 0.8863656520843506\n",
      "step 60 loss: 0.8686092495918274\n",
      "step 80 loss: 0.7246319651603699\n",
      "step 100 loss: 0.8080551028251648\n",
      "step 120 loss: 0.9705864191055298\n",
      "step 140 loss: 0.8428952693939209\n",
      "step 160 loss: 0.7104964256286621\n",
      "step 180 loss: 0.6005547046661377\n",
      "\n",
      "R2: 0.6275092623172558\t MSE: 0.9730581\n",
      "use time: 21.5932195186615\n",
      "----------------------------------------\n",
      "epoch: 21\n",
      "step 0 loss: 0.8925215601921082\n",
      "step 20 loss: 0.8271934986114502\n",
      "step 40 loss: 0.9571546316146851\n",
      "step 60 loss: 0.8554040789604187\n",
      "step 80 loss: 0.6992256045341492\n",
      "step 100 loss: 0.80787593126297\n",
      "step 120 loss: 0.8165981769561768\n",
      "step 140 loss: 0.8529157638549805\n",
      "step 160 loss: 0.6278096437454224\n",
      "step 180 loss: 0.6770257949829102\n",
      "\n",
      "R2: 0.6297062836119742\t MSE: 0.9673188\n",
      "use time: 21.704896688461304\n",
      "----------------------------------------\n",
      "epoch: 22\n",
      "step 0 loss: 0.8333366513252258\n",
      "step 20 loss: 0.7658573389053345\n",
      "step 40 loss: 0.8342553973197937\n",
      "step 60 loss: 0.8261657357215881\n",
      "step 80 loss: 0.675330400466919\n",
      "step 100 loss: 0.7837945222854614\n",
      "step 120 loss: 0.9051406383514404\n",
      "step 140 loss: 0.8935509920120239\n",
      "step 160 loss: 0.6775283813476562\n",
      "step 180 loss: 0.6056705713272095\n",
      "\n",
      "R2: 0.6203396096554562\t MSE: 0.9917874\n",
      "use time: 21.697960138320923\n",
      "----------------------------------------\n",
      "epoch: 23\n",
      "step 0 loss: 0.9026404619216919\n",
      "step 20 loss: 0.667794942855835\n",
      "step 40 loss: 0.8432846069335938\n",
      "step 60 loss: 0.8353548049926758\n",
      "step 80 loss: 0.6873348951339722\n",
      "step 100 loss: 0.7049673795700073\n",
      "step 120 loss: 0.971901535987854\n",
      "step 140 loss: 0.7653659582138062\n",
      "step 160 loss: 0.6722463369369507\n",
      "step 180 loss: 0.666549563407898\n",
      "\n",
      "R2: 0.6511913222968505\t MSE: 0.9111934\n",
      "use time: 21.614880084991455\n",
      "----------------------------------------\n",
      "epoch: 24\n",
      "step 0 loss: 0.8921359181404114\n",
      "step 20 loss: 0.7881013751029968\n",
      "step 40 loss: 0.8598155975341797\n",
      "step 60 loss: 0.76624596118927\n",
      "step 80 loss: 0.6542375087738037\n",
      "step 100 loss: 0.712658166885376\n",
      "step 120 loss: 0.9032413959503174\n",
      "step 140 loss: 0.8737517595291138\n",
      "step 160 loss: 0.710489809513092\n",
      "step 180 loss: 0.636588990688324\n",
      "\n",
      "R2: 0.6451047611391258\t MSE: 0.9270933\n",
      "use time: 21.75438380241394\n",
      "----------------------------------------\n",
      "epoch: 25\n",
      "step 0 loss: 0.781710147857666\n",
      "step 20 loss: 0.7287617921829224\n",
      "step 40 loss: 0.9094648957252502\n",
      "step 60 loss: 0.8498919010162354\n",
      "step 80 loss: 0.6911910772323608\n",
      "step 100 loss: 0.6870377063751221\n",
      "step 120 loss: 0.9439855217933655\n",
      "step 140 loss: 0.8286948204040527\n",
      "step 160 loss: 0.5969399213790894\n",
      "step 180 loss: 0.5849125385284424\n",
      "\n",
      "R2: 0.65351743140214\t MSE: 0.9051168\n",
      "use time: 21.595948696136475\n",
      "----------------------------------------\n",
      "epoch: 26\n",
      "step 0 loss: 0.8415786623954773\n",
      "step 20 loss: 0.6591135263442993\n",
      "step 40 loss: 0.8100613355636597\n",
      "step 60 loss: 0.848595380783081\n",
      "step 80 loss: 0.6338477730751038\n",
      "step 100 loss: 0.715956449508667\n",
      "step 120 loss: 0.8545418977737427\n",
      "step 140 loss: 0.761543869972229\n",
      "step 160 loss: 0.655612587928772\n",
      "step 180 loss: 0.7033644318580627\n",
      "\n",
      "R2: 0.6515619524045122\t MSE: 0.9102252\n",
      "use time: 21.674043655395508\n",
      "----------------------------------------\n",
      "epoch: 27\n",
      "step 0 loss: 0.8689872026443481\n",
      "step 20 loss: 0.7179041504859924\n",
      "step 40 loss: 0.7635904550552368\n",
      "step 60 loss: 0.7943681478500366\n",
      "step 80 loss: 0.624732255935669\n",
      "step 100 loss: 0.7441461682319641\n",
      "step 120 loss: 0.7623428106307983\n",
      "step 140 loss: 0.7901137471199036\n",
      "step 160 loss: 0.6293524503707886\n",
      "step 180 loss: 0.5873715877532959\n",
      "\n",
      "R2: 0.6473644893536993\t MSE: 0.92119026\n",
      "use time: 21.645731687545776\n",
      "----------------------------------------\n",
      "epoch: 28\n",
      "step 0 loss: 0.7225746512413025\n",
      "step 20 loss: 0.685447096824646\n",
      "step 40 loss: 0.8197652101516724\n",
      "step 60 loss: 0.7784070372581482\n",
      "step 80 loss: 0.6565313935279846\n",
      "step 100 loss: 0.6367549896240234\n",
      "step 120 loss: 0.7817598581314087\n",
      "step 140 loss: 0.7257726192474365\n",
      "step 160 loss: 0.6179224252700806\n",
      "step 180 loss: 0.5952768325805664\n",
      "\n",
      "R2: 0.670470205588334\t MSE: 0.8608312\n",
      "use time: 21.699196100234985\n",
      "----------------------------------------\n",
      "epoch: 29\n",
      "step 0 loss: 0.778745174407959\n",
      "step 20 loss: 0.6936119794845581\n",
      "step 40 loss: 0.8777446150779724\n",
      "step 60 loss: 0.7327049374580383\n",
      "step 80 loss: 0.6076856851577759\n",
      "step 100 loss: 0.6434942483901978\n",
      "step 120 loss: 0.8589502573013306\n",
      "step 140 loss: 0.7446564435958862\n",
      "step 160 loss: 0.5717556476593018\n",
      "step 180 loss: 0.5880889296531677\n",
      "\n",
      "R2: 0.6433228993116591\t MSE: 0.9317481\n",
      "use time: 21.89867067337036\n",
      "----------------------------------------\n",
      "epoch: 30\n",
      "step 0 loss: 0.8026896119117737\n",
      "step 20 loss: 0.7170416116714478\n",
      "step 40 loss: 0.7744268178939819\n",
      "step 60 loss: 0.7400696277618408\n",
      "step 80 loss: 0.6105084419250488\n",
      "step 100 loss: 0.6721706390380859\n",
      "step 120 loss: 0.7653146982192993\n",
      "step 140 loss: 0.7529292702674866\n",
      "step 160 loss: 0.5676846504211426\n",
      "step 180 loss: 0.5895475149154663\n",
      "\n",
      "R2: 0.6577984123378864\t MSE: 0.8939337\n",
      "use time: 21.660398483276367\n",
      "----------------------------------------\n",
      "epoch: 31\n",
      "step 0 loss: 0.7327286005020142\n",
      "step 20 loss: 0.7326079607009888\n",
      "step 40 loss: 0.7885992527008057\n",
      "step 60 loss: 0.7562961578369141\n",
      "step 80 loss: 0.6462503671646118\n",
      "step 100 loss: 0.6442656517028809\n",
      "step 120 loss: 0.8207886219024658\n",
      "step 140 loss: 0.6404128074645996\n",
      "step 160 loss: 0.5907946825027466\n",
      "step 180 loss: 0.5661875009536743\n",
      "\n",
      "R2: 0.6610120167499258\t MSE: 0.8855388\n",
      "use time: 21.570571899414062\n",
      "----------------------------------------\n",
      "epoch: 32\n",
      "step 0 loss: 0.6777832508087158\n",
      "step 20 loss: 0.7208113670349121\n",
      "step 40 loss: 0.7802658081054688\n",
      "step 60 loss: 0.8081867098808289\n",
      "step 80 loss: 0.5956442356109619\n",
      "step 100 loss: 0.6205398440361023\n",
      "step 120 loss: 0.8167915344238281\n",
      "step 140 loss: 0.8191044330596924\n",
      "step 160 loss: 0.5701422095298767\n",
      "step 180 loss: 0.600824236869812\n",
      "\n",
      "R2: 0.6817812041137601\t MSE: 0.8312834\n",
      "use time: 21.62746286392212\n",
      "----------------------------------------\n",
      "epoch: 33\n",
      "step 0 loss: 0.7643889784812927\n",
      "step 20 loss: 0.6532906293869019\n",
      "step 40 loss: 0.7202266454696655\n",
      "step 60 loss: 0.661307692527771\n",
      "step 80 loss: 0.6144673228263855\n",
      "step 100 loss: 0.6817774772644043\n",
      "step 120 loss: 0.780095100402832\n",
      "step 140 loss: 0.7514532804489136\n",
      "step 160 loss: 0.513663113117218\n",
      "step 180 loss: 0.5681751370429993\n",
      "\n",
      "R2: 0.6723570296658921\t MSE: 0.8559022\n",
      "use time: 21.660020351409912\n",
      "----------------------------------------\n",
      "epoch: 34\n",
      "step 0 loss: 0.7516043186187744\n",
      "step 20 loss: 0.5988542437553406\n",
      "step 40 loss: 0.6956721544265747\n",
      "step 60 loss: 0.7949216365814209\n",
      "step 80 loss: 0.5343351364135742\n",
      "step 100 loss: 0.701339066028595\n",
      "step 120 loss: 0.7037813663482666\n",
      "step 140 loss: 0.7085723280906677\n",
      "step 160 loss: 0.5644590854644775\n",
      "step 180 loss: 0.5935784578323364\n",
      "\n",
      "R2: 0.6868297432467956\t MSE: 0.81809515\n",
      "use time: 21.789053916931152\n",
      "----------------------------------------\n",
      "epoch: 35\n",
      "step 0 loss: 0.6847736239433289\n",
      "step 20 loss: 0.6210024952888489\n",
      "step 40 loss: 0.7660970091819763\n",
      "step 60 loss: 0.68901526927948\n",
      "step 80 loss: 0.549829363822937\n",
      "step 100 loss: 0.6704513430595398\n",
      "step 120 loss: 0.7357413172721863\n",
      "step 140 loss: 0.7294321060180664\n",
      "step 160 loss: 0.5294332504272461\n",
      "step 180 loss: 0.5252722501754761\n",
      "\n",
      "R2: 0.6796466325006192\t MSE: 0.8368596\n",
      "use time: 21.664396047592163\n",
      "----------------------------------------\n",
      "epoch: 36\n",
      "step 0 loss: 0.7825912237167358\n",
      "step 20 loss: 0.6855758428573608\n",
      "step 40 loss: 0.7508730888366699\n",
      "step 60 loss: 0.6806573867797852\n",
      "step 80 loss: 0.4407023787498474\n",
      "step 100 loss: 0.7075125575065613\n",
      "step 120 loss: 0.7678433656692505\n",
      "step 140 loss: 0.6350770592689514\n",
      "step 160 loss: 0.5420143604278564\n",
      "step 180 loss: 0.5176417827606201\n",
      "\n",
      "R2: 0.6871840663256286\t MSE: 0.81716955\n",
      "use time: 21.69291067123413\n",
      "----------------------------------------\n",
      "epoch: 37\n",
      "step 0 loss: 0.7675248980522156\n",
      "step 20 loss: 0.7234741449356079\n",
      "step 40 loss: 0.7094056606292725\n",
      "step 60 loss: 0.6599809527397156\n",
      "step 80 loss: 0.5158045291900635\n",
      "step 100 loss: 0.6243991851806641\n",
      "step 120 loss: 0.7256863713264465\n",
      "step 140 loss: 0.6719084978103638\n",
      "step 160 loss: 0.5537937879562378\n",
      "step 180 loss: 0.5390824675559998\n",
      "\n",
      "R2: 0.69182733503631\t MSE: 0.8050399\n",
      "use time: 21.68594241142273\n",
      "----------------------------------------\n",
      "epoch: 38\n",
      "step 0 loss: 0.6819983720779419\n",
      "step 20 loss: 0.6071878671646118\n",
      "step 40 loss: 0.7306609749794006\n",
      "step 60 loss: 0.6056546568870544\n",
      "step 80 loss: 0.4592846632003784\n",
      "step 100 loss: 0.6380377411842346\n",
      "step 120 loss: 0.6773639917373657\n",
      "step 140 loss: 0.6790822148323059\n",
      "step 160 loss: 0.5063140392303467\n",
      "step 180 loss: 0.5713049173355103\n",
      "\n",
      "R2: 0.68909584982856\t MSE: 0.8121753\n",
      "use time: 21.683053016662598\n",
      "----------------------------------------\n",
      "epoch: 39\n",
      "step 0 loss: 0.7074036598205566\n",
      "step 20 loss: 0.7019363641738892\n",
      "step 40 loss: 0.703606128692627\n",
      "step 60 loss: 0.5580359697341919\n",
      "step 80 loss: 0.5040632486343384\n",
      "step 100 loss: 0.6200460195541382\n",
      "step 120 loss: 0.7120215892791748\n",
      "step 140 loss: 0.5963561534881592\n",
      "step 160 loss: 0.48973381519317627\n",
      "step 180 loss: 0.5670410394668579\n",
      "\n",
      "R2: 0.6738032843511713\t MSE: 0.85212415\n",
      "use time: 21.64096212387085\n",
      "----------------------------------------\n",
      "epoch: 40\n",
      "step 0 loss: 0.656252384185791\n",
      "step 20 loss: 0.6224892735481262\n",
      "step 40 loss: 0.6705251932144165\n",
      "step 60 loss: 0.6790789365768433\n",
      "step 80 loss: 0.4974086284637451\n",
      "step 100 loss: 0.7117427587509155\n",
      "step 120 loss: 0.6820616722106934\n",
      "step 140 loss: 0.5828843116760254\n",
      "step 160 loss: 0.5046663284301758\n",
      "step 180 loss: 0.5555046796798706\n",
      "\n",
      "R2: 0.6919663729116192\t MSE: 0.8046767\n",
      "use time: 21.81697940826416\n",
      "----------------------------------------\n",
      "epoch: 41\n",
      "step 0 loss: 0.6296495795249939\n",
      "step 20 loss: 0.5966636538505554\n",
      "step 40 loss: 0.6963130235671997\n",
      "step 60 loss: 0.6231085062026978\n",
      "step 80 loss: 0.42353975772857666\n",
      "step 100 loss: 0.5375116467475891\n",
      "step 120 loss: 0.7328670024871826\n",
      "step 140 loss: 0.6697134971618652\n",
      "step 160 loss: 0.5230612754821777\n",
      "step 180 loss: 0.46505099534988403\n",
      "\n",
      "R2: 0.6967962398172438\t MSE: 0.79205954\n",
      "use time: 21.715275526046753\n",
      "----------------------------------------\n",
      "epoch: 42\n",
      "step 0 loss: 0.6369408369064331\n",
      "step 20 loss: 0.6251102685928345\n",
      "step 40 loss: 0.6663975715637207\n",
      "step 60 loss: 0.6435860395431519\n",
      "step 80 loss: 0.4161967635154724\n",
      "step 100 loss: 0.6092163324356079\n",
      "step 120 loss: 0.7353591918945312\n",
      "step 140 loss: 0.6167862415313721\n",
      "step 160 loss: 0.559435248374939\n",
      "step 180 loss: 0.47554877400398254\n",
      "\n",
      "R2: 0.6952696497608829\t MSE: 0.7960475\n",
      "use time: 21.656868934631348\n",
      "----------------------------------------\n",
      "epoch: 43\n",
      "step 0 loss: 0.5780296325683594\n",
      "step 20 loss: 0.5130693912506104\n",
      "step 40 loss: 0.5910412073135376\n",
      "step 60 loss: 0.5521560907363892\n",
      "step 80 loss: 0.4763760268688202\n",
      "step 100 loss: 0.5024416446685791\n",
      "step 120 loss: 0.6843793392181396\n",
      "step 140 loss: 0.566405713558197\n",
      "step 160 loss: 0.426934152841568\n",
      "step 180 loss: 0.4497845470905304\n",
      "\n",
      "R2: 0.6819337633604408\t MSE: 0.8308849\n",
      "use time: 21.74674153327942\n",
      "----------------------------------------\n",
      "epoch: 44\n",
      "step 0 loss: 0.5952630043029785\n",
      "step 20 loss: 0.5482611656188965\n",
      "step 40 loss: 0.6635504364967346\n",
      "step 60 loss: 0.6192828416824341\n",
      "step 80 loss: 0.49618640542030334\n",
      "step 100 loss: 0.5997697710990906\n",
      "step 120 loss: 0.6803950071334839\n",
      "step 140 loss: 0.6173783540725708\n",
      "step 160 loss: 0.5135968923568726\n",
      "step 180 loss: 0.4930921494960785\n",
      "\n",
      "R2: 0.6882481978558219\t MSE: 0.81438965\n",
      "use time: 21.579526901245117\n",
      "----------------------------------------\n",
      "epoch: 45\n",
      "step 0 loss: 0.6705149412155151\n",
      "step 20 loss: 0.5249835848808289\n",
      "step 40 loss: 0.7078876495361328\n",
      "step 60 loss: 0.6471097469329834\n",
      "step 80 loss: 0.5021932721138\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-75163086b613>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtime0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mavgloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mmsescore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr2score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_mol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_classify\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-106-bfc11eee6127>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(model, train_dataloader, lossfunc, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mloop_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wbm001/miniconda3/envs/DeepPurpose/lib/python3.6/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wbm001/miniconda3/envs/DeepPurpose/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "\n",
    "writer = SummaryWriter(tensorboard_path)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    print(\"--\"*20)\n",
    "    print(\"epoch: \" + str(epoch))\n",
    "    time0 = time.time()\n",
    "\n",
    "    avgloss = train_loop(model, trainDataLoader, loss_fn, optimizer, scheduler)\n",
    "    msescore, r2score = test_loop(model, val_mol, val_seq, val_classify, writer, epoch)\n",
    "\n",
    "    writer.add_scalar(\"test time\", time.time()-time0, epoch)\n",
    "    writer.add_scalar('avgloss', avgloss , epoch)\n",
    "    writer.add_scalar('mse', msescore , epoch)\n",
    "    writer.add_scalar('r2', r2score , epoch)\n",
    "    writer.add_scalar('current lr', optimizer.param_groups[0]['lr'], epoch)\n",
    "\n",
    "    print()\n",
    "    print(\"R2: \" + str(r2score) + \"\\t MSE: \" + str(msescore))\n",
    "    print(\"use time: \" + str(time.time() - time0))\n",
    "    \n",
    "    model.eval()\n",
    "    if epoch % 50 == 0:\n",
    "        torch.save({'state_dict': model.state_dict()}, data_path + 'model/' + str(version) + \"e\" + str(epoch) + '.pth.tar')\n",
    "    else:\n",
    "        torch.save({'state_dict': model.state_dict()}, data_path + \"model/quicksave.pth.tar\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('DeepPurpose')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f923ce3d9a6c852fa4277c7633c15be33e3c2fd747753029b4f9b6323264f49d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
